# -*- coding: utf-8 -*-
"""model1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c5IGMxRlSfUZ9b7vjhN3QYmmQ0W554Oc
"""

import numpy as np
import pandas as pd
import sklearn
import matplotlib.pyplot as plt
import nltk

df=pd.read_csv('big_spam_dataset.csv')


df.rename(columns={'label':'target'},inplace=True)

df.head()

df=df.drop_duplicates(keep='first')
df.duplicated().sum()
df.shape

nltk.download('punkt')

df['target'].value_counts()
plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct='%0.2f')
plt.show



nltk.download('punkt_tab')
df['num_char']=df['text'].apply(len)

df['num_words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))


df['num_sent']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

from nltk.corpus import stopwords
nltk.download('stopwords')
import string
from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()

def transform_data(text):
    text=text.lower()

    text=nltk.word_tokenize(text)
    y=[]
    for i in text:
        if i.isalnum():
            y.append(i)

    text=y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text=y[:]
    y.clear()

    for i in text:
        y.append(ps.stem(i))
    return " ".join(y)

df['transform_text']=df['text'].apply(transform_data)
df.head()

from wordcloud import WordCloud
wc=WordCloud()

spam_wc=wc.generate(df[df['target']==1]['transform_text'].str.cat(sep=" "))
plt.imshow(spam_wc)

ham_wc=wc.generate(df[df['target']==0]['transform_text'].str.cat(sep=" "))
plt.imshow(ham_wc)

spam_corpus=[]
for i in df[df['target']==1]['transform_text'].tolist():
  for word in i.split():
    spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
Counter(spam_corpus).most_common(30)

"""model bulding

"""

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv=CountVectorizer()
tfif=TfidfVectorizer()

x=tfif.fit_transform(df['transform_text']).toarray()

x.shape

y=df['target']

y

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB

from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

mnb=MultinomialNB()
mnb.fit(x_train,y_train)
y_pred=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(precision_score(y_test,y_pred))

gbn=GaussianNB()
gbn.fit(x_train,y_train)
y_pred1=gbn.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

bn=BernoulliNB()

bn.fit(x_train,y_train)
y_pred2=bn.predict(x_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

import pickle

# Example: save model as pickle
with open("spam_model.pkl", "wb") as f:
    pickle.dump(mnb, f)

# Download in local system
from google.colab import files
files.download("spam_model.pkl")

pickle.dump(tfif,open('vectorizer.pkl','wb'))

